import { BASE_URL } from "@/lib/seo";

/**
 * Dynamic robots.txt — generated by Next.js at request time.
 * Accessible at: https://re-e.dk/robots.txt
 *
 * What this file does:
 *  - Tells search engine crawlers (Google, Bing, etc.) which pages to index.
 *  - Has NO effect on real users — anyone can still visit any page.
 *
 * Rules:
 *  - Allow: all public-facing pages including login/register
 *    (the home page redirects to login, so it must be indexable)
 *  - Disallow: private user areas, admin panel, and internal API routes
 *  - Disallow: cart/checkout/thankyou (transactional, no SEO value)
 */
export default function robots() {
  return {
    rules: [
      {
        userAgent: "*",
        allow: [
          "/",
          "/product/",
          "/login",
          "/register",
          "/contact-support",
          "/privacy-policy",
          "/data-deletion",
        ],
        disallow: [
          "/dashboard/",
          "/admin/",
          "/api/",
          "/cart",
          "/checkout",
          "/thankyou",
          "/forgot-password",
          "/reset-password",
        ],
      },
    ],
    sitemap: `${BASE_URL}/sitemap.xml`,
    host: BASE_URL,
  };
}
